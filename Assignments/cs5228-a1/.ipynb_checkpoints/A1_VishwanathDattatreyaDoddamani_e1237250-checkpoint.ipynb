{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d51ef5-f347-42f2-98d9-56a68f9f2988",
   "metadata": {},
   "source": [
    "<img src=\"images/cs5228-header-title.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350fe03b",
   "metadata": {},
   "source": [
    "# Assignment 1 - Exploratory Data Analysis (EDA) & K-Means\n",
    "\n",
    "Hello everyone, this assignment notebook covers Exploratory Data Analysis (EDA) and K-Means. There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A1_YourName_YourNUSNETID.ipynb** (e.g., **A1_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Remember to rename and save the Python script file **A1_YourName_YourNUSNETID.py** (e.g., **A1_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is **Sep 12, 11.59 pm**. Late submissions will be penalized by 10% for each additional day. There is no need to use your full name if it's rather long; it's just important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0286188L'\n",
    "nusnet_id = 'E1237250'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479007c",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well documented, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 Data Cleaning & Exploratory Data Analysis (EDA) (30 Points)**\n",
    "    * 1.1 Preparing a Real-World Dataset for Analysis (22 Points)\n",
    "        * 1.1 a) Removing \"Dirty\" Records (6 Points)\n",
    "        * 1.1 b) Handling Missing (NaN) Values (6 Points)\n",
    "        * 1.1 c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "        * 1.1 d) Handling of Categorical Attributes (4 Points)\n",
    "    * 1.2 Basic Facts about a Real-World Dataset (8 Points)\n",
    "* **2 K-Means (18 Points)**\n",
    "    * 2.1 Implementing K-Means++ (12 Points)\n",
    "        * 2.1 a) Initializing Centroids Based on K-Means++ (5 Points)\n",
    "        * 2.1 b) Assigning Data Points to Clusters (4 Points)\n",
    "        * 2.1 c) Updating the Centroids (3 Points)\n",
    "    * 2.2 Questions about K-Means (8 Points)\n",
    "        * 2.2 a) Understanging K-Means++ Initialization (6 Points)\n",
    "        * 2.2 b) K-Means with Different Distance/Similarity metrics (2 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8ffdec",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "954663a3-9ac0-4587-8d12-17f6543cd37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some magic so that the notebook will reload the external python script file any time you edit and save the .py file;\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a41bf-c5b7-43d2-986b-d4a8f9aae17c",
   "metadata": {},
   "source": [
    "Making all the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb7b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fa18ac",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A1.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from A1 import clean, handle_nan, extract_facts, MyKMeans\n",
    "#from A1_BobSmith_e12345678 import lean, handle_nan, extract_facts, MyKMeans # <-- you well need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38541ac6",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dead6f",
   "metadata": {},
   "source": [
    "# 1 Data Cleaning & Explorative Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85572cfd",
   "metadata": {},
   "source": [
    "### 1.1 Preparing a Real-World Dataset for Analysis (22 Points)\n",
    "\n",
    "For the following tasks, we consider a dataset containing information for 20,000 past resale transactions of condo flats. Each record (i.e., data samples) consists of 12 attributes. The following **data description** list all attributes together with a brief description of each attribute's data type / domain:\n",
    "\n",
    "* **transaction_id**: Unique ID of the resale transactions; an 8-digit integer number uniquely assigned to each transaction.\n",
    "* **url**: Unique link to a website documenting this transaction as a string value.\n",
    "* **name**: The name of the condo as a string value (e.g., \"estella gardens\", \"eedon green\").\n",
    "* **type**: The type of condo as a string value (e.g., \"condominium\", \"apartment\").\n",
    "* **postal_district**: The postal district the condo is located in as an integer value; Singapore has 28 postal districts: 1, 2, ..., 28 (cf. [here](https://www.propertygiant.com/resource/singapore-postal-districts-map)).\n",
    "* **subzone**: The subzone the condo is located in as a string value.\n",
    "* **planning_area**: The planning area the condo is located in as a string value.\n",
    "* **date_of_sale**: The date (month & year) of the transaction as a string value (e.g., \"mar-19\", \"oct-20\").\n",
    "* **area_sqft**: The size of the condo flat in square feet as a positive integer value.\n",
    "* **floor_level**: The range of floors in which the flat is located in the condo as a string value (e.g., \"06 to 10\", \"11 to 15\").\n",
    "* **eco_category**: The eco category of the condo as a single-character string value (e.g., \"A\", \"B\", \"C\", \"D\").\n",
    "* **price**: Resale price of the condo flat in Singapore Dollar as an integer value.\n",
    "\n",
    "Additional information: Singapore has 55 panning areas; each planning area is further split into multiple subzones (if you are interested, you can check out the corresponding [Wikipedia article](https://en.wikipedia.org/wiki/List_of_places_in_Singapore)).\n",
    "\n",
    "**Important:** In each of the following subtask 1.1 a)-c) we use a slightly different version of the dataset. This allows you to focus on the specific aspects of data cleaning / data preprocessing addressed in the respective subtask. In practice, of course, there would be only one dataset containing all the different issues (e.g., \"dirty\" records, missing data, etc.) at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e96b7",
   "metadata": {},
   "source": [
    "#### 1.1 a) Removing \"Dirty\" Records (6 Points)\n",
    "\n",
    "We argued in the lecture that almost all real-world datasets contain some form of noise that might negatively affect any applied data analysis. The very first -- and in some sense -- easiest way to identify noise is to check if all data confirms with the data description. The following code cell shows a snippet of the dataset which you will be looking at in this subtask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_dirty = pd.read_csv('data/a1-condo-resale-dirty.csv')\n",
    "\n",
    "df_condos_dirty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f68ad",
   "metadata": {},
   "source": [
    "If you check the dataset against its description as given above -- with the help of `pandas` or by simply inspecting the raw data file -- you will notice that many records are \"dirty\", meaning they are not in the expected format. Dirty records can negatively affect any subsequent analysis it needs.\n",
    "\n",
    "**Perform EDA on the Condo dataset and perform appropriate preprocessing steps to clean the data!** The preprocessing step for cleaning the data step may include\n",
    "* the *removal* of \"dirty\" records or\n",
    "* the *modification* of \"dirty\" records records\n",
    "\n",
    "**Task 1: Identify at least 3 issues** with the dataset that would negatively affect any subsequent analysis due to \"dirty\" records, and clean the data accordingly.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "* Recall from the lecture that data cleaning often involves making certain decisions. As such, you might come up with different steps than other students. This is OK as long as you can reasonably justify your steps.\n",
    "* The goal is to preserve as much of the records as possible! So only remove records as part of your data cleaning if it's really necessary (this includes that you should not remove any attributes!). There might be different valid cases, so don't forget to briefly justify your decision.\n",
    "* For this task, we are **not** trying to identify \"noise\" (e.g., outliers in the form of suspicious values) or `NaN` values; this will be covered later. Here, we look for records with values not eligible to be used for an analysis because they do not adhere to the data description!\n",
    "\n",
    "Please provide your answer below. It should list the different issues you have identified and briefly discuss which data cleaning steps you can and/or need to perform to address those issues.\n",
    "\n",
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fb36c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c2faf0c",
   "metadata": {},
   "source": [
    "**Task 2: Complete the method `clean` in the `.py` file to actually implement your steps for handling \"dirty records\"!** The results should back up your answer above. *Hint:* Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_cleaned = clean(df_condos_dirty)\n",
    "\n",
    "print('After preprocessing, there are now {} records.'.format(df_condos_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cb155",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after the cleaning step as there is some wiggle room regarding the performed steps which would affect this result. As such, even if two solutions are correct, they do not necessarily yield the same number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bac07c",
   "metadata": {},
   "source": [
    "#### 1.1 b) Handling Missing (NaN) Values (6 Points)\n",
    "\n",
    "Many traditional data mining algorithms do not like missing (NaN) values and will throw an error if missing values are present. We therefore have to address missing values and get rid of them. On the other hand, we want to preserve as much of our dataset as possible, so we need to be smart about that. In this subtask, you are provided with a version of our condos resale dataset that contains missing values but is otherwise clean -- so it is all about the `NaN` values here.\n",
    "\n",
    "Let's load the dataset and have a quick look -- the attributes are the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2468397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_nan = pd.read_csv('data/a1-condo-resale-nan.csv')\n",
    "\n",
    "df_condos_nan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d558f5",
   "metadata": {},
   "source": [
    "Since your decision for handling `NaN` values might depend in the data mining task, assume in the following that you want to use this dataset to **create a regression model to predict the resale price** from the attributes of a transaction. Of course, there will be no need to actually create such a model here.\n",
    "\n",
    "**Task 1: Identify all `NaN` values in the dataset and handle them appropriately!** After this preprocessing, the resulting dataset should no longer contain any `NaN` values. Please provide your answer in the markdown cell below. List all issues concerning `NaN` values and how you would handle them with a brief justification for your approach. Additional (simplifying) guidelines:\n",
    "\n",
    "* The goal is to preserve as many records in the dataset as possible.\n",
    "* You do not need to consider external knowledge (i.e., information coming from outside this dataset)\n",
    "* Ignore more sophisticated solutions such as [`sklearn.impute.KNNImputer`](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html). These can be very useful in practice (and maybe for your project), but their application requires certain assumptions to hold for good results. This is beyond the scope of this assignment.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d28cdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0be8e11",
   "metadata": {},
   "source": [
    "**Task 2: Complete the method `handle_nan` in the `.py` file to actually implement your steps for handling `NaN` values!** The results should back up your answer above. Hint: Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_no_nan = handle_nan(df_condos_nan)\n",
    "\n",
    "print('After handling missing values, there are now {} records.'.format(df_condos_no_nan.shape[0]))\n",
    "print('Number of records with an NaN for any attribute: {}'.format((df_condos_no_nan.isna().sum(axis=1) > 0).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d529c",
   "metadata": {},
   "source": [
    "**Important:** We do not provide an expected output regarding the number of records after this preprocessing step as there is some wiggle room regarding the performed steps which would affect this result. However, the number of records with `NaN` values should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caddf10",
   "metadata": {},
   "source": [
    "#### 1.1 c) Other Appropriate Data Cleaning / Preprocessing Steps (6 Points)\n",
    "\n",
    "Identifying \"dirty\" records and missing data are two very fundamental and generally rather systematic steps as part of data cleaning / data preprocessing. However, as we saw in the lecture using some examples, there are many other issues with the dataset that can be considered noise and thus potentially negatively affecting any data analysis. So the more noise we can remove, the more likely we can expect meaningful analysis results.\n",
    "\n",
    "For this subtask, we use a version of our condo resale dataset **with no \"dirty\" records or missing data**! If you think a record should be considered \"dirty\", simply ignore it for this task! Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407dd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_others = pd.read_csv('data/a1-condo-resale-others.csv')\n",
    "\n",
    "df_condos_others.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b654ee93",
   "metadata": {},
   "source": [
    "**List all data cleaning / data preprocessing steps you deem appropriate to apply to the dataset above!** Please provide your answer in the markdown cell below list all steps together with a justification for your decision. Additional (simplifying) guidelines:\n",
    "\n",
    "* You should still assume that we want to use this dataset to create a model for predicting the resale price of a flat based on its attributes. The choice of data mining task is very likely to affect your decision for what cleaning / preprocessing steps to apply.\n",
    "* There is no need to consider external knowledge. For example, you do not have to check if a value for `subzone` is indeed an existing subzone of Singapore.\n",
    "* Please do not address the concept of attribute encoding -- that is, the encoding of categorical attributes as numerical attributes -- in this subtask. This comes later.\n",
    "* There is no need for you to implement any processing steps! Most important are your justifications for your decisions.\n",
    "\n",
    "**Your Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e311bc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "663872f4",
   "metadata": {},
   "source": [
    "#### 1.1 d) Handling Categorical Attributes (4 Points)\n",
    "\n",
    "Many to most data mining algorithms require all input features / attributes to be numerical. Our dataset with transactions resales of condo flats contains attributes that are not all numerical. As such, assuming we indeed want to utilize them, we need to convert those attributes into numerical ones. Regarding encoding techniques, we covered [One-Hot Encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) in the lecture, and also talked about [Target Encoding](https://contrib.scikit-learn.org/category_encoders/targetencoder.html) in the tutorial; you can and should also look into other encoding techniques.\n",
    "\n",
    "**Briefly(!) discuss how you would handle each categorical attribute and justify why!** Handling a categorical attribute means\n",
    "* to drop a categorical attribute *or*\n",
    "* to encode it into a numerical representation *or*\n",
    "* to otherwise transform it into a numerical attribute\n",
    "\n",
    "There is no single correct answer for this task; it's your justification that matters. Again, assume that we want to create a regression model to predict the resale price of a flat based on the other features.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf85c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87e53a69",
   "metadata": {},
   "source": [
    "### 1.2 Basic Facts about a Real-World Dataset (8 Points)\n",
    "\n",
    "The following tasks are about getting basic insights into the Condo Resale Prices dataset. As the data preprocessing steps you choose to perform might affect the results of this task, we will use a modified version here. Note that this version contains 50,000 listing of condo resale transactions and does **not** contain any \"dirty\" records. This is to ensure that everyone uses the same data. This helps marking your solutions as we know which results to expect. If you think a record might should be considered \"dirty\", simply ignore it for this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_condos_facts = pd.read_csv('data/a1-condo-resale-facts.csv')\n",
    "\n",
    "df_condos_facts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fbfe24",
   "metadata": {},
   "source": [
    "Please complete the table below by answering the 7 given questions. Use the code cell below the table to actually implement your steps that enabled you to answer the questions. There is no need for a fancy layout for any print statement; it's only important that the result is clear.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff81b3",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answers for (1)-(7). Answers (1)-(6) are worth 1 Point each; Answer (7) is worth 2 Points.\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| (1)  | What is the date (month & year) of the earliest transactions? | ??? |\n",
    "| (2)  | For each `type`, how many transactions are in the dataset?  | ??? |\n",
    "| (3)  | How many condo apartments in Redhill (*subzone*) costing more than SGD 2,000,000 have been sold? | ??? |\n",
    "| (4)  | What is the planning area with the most number of transactions? List the name of the planning area and the number of transactions!  | ??? |\n",
    "| (5)  | Which transaction in postal district 11 had the highest price-to-area ratio (i.e., the highest price per square foot)? List the name of the condo and the price per square foot (rounded to 2 decimals) | ??? |\n",
    "| (6)  | What is the correlation between the resale *price* and *area_sqft*? | ??? |\n",
    "| (7)  | What is the number of transactions where the flat/apartment was between the 50th floor (inclusive) and 60th (inclusive) floor?  | ??? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468a32c-6a9d-432b-9204-90d58b8e49f3",
   "metadata": {},
   "source": [
    "**Complete the method `extract_facts()` in the `.py` file to actually extract the facts!** The results should back up your answer above. *Hint:* Try to use methods/functionalities provided by `pandas` as much as possible; it will make your code much shorter, faster and your life easier. We will run performance checks for this method to evaluate the runtime, but the performance is not considered for the grading!\n",
    "\n",
    "**Note:** You can simply use simple print statements that somehow show the result you entered into the table above. You do not have to ensure any specific output format. It should only *somehow* be possible to match the answers you have added to the table above to the output of the code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9313079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_facts(df_condos_facts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323314b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b46bc-4d06-4d67-87cf-db440d0d02ac",
   "metadata": {},
   "source": [
    "## 2 K-Means\n",
    "\n",
    "K-Means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal is to group data points into clusters such that points within the same cluster are more similar to each other than to those in other clusters, based on their feature similarity. It operates iteratively to assign each data point to the nearest of K centroids (representative points of clusters) and then recalculates the centroids based on the mean of the points assigned to each cluster. This process continues until convergence, typically defined by minimal change in centroids or assignment of data points. K-Means is efficient and straightforward, making it well-suited for many clustering tasks in various fields such as image processing, document classification, and customer segmentation.\n",
    "\n",
    "### Dataset Preparation (nothing for you to do here)\n",
    "\n",
    "For easy testing and debugging your implementation of K-Means++, we provide you with a simple 2-dimensional dataset containing 100 data points. Just by looking at the plot, one can argue that there are six clusters; although that is not important for the testing and debugging. Later, you can try different values for $k$ and visualize the result using a method we provide as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854d302-e95d-473c-a17b-ea59851a5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kmeans = pd.read_csv('data/a1-kmeans-toy-data.csv', header=None).to_numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_kmeans[:,0], X_kmeans[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d3349-7715-43b6-91ed-c5d757c1267e",
   "metadata": {},
   "source": [
    "### 2.1 Implementing K-Means++ (12 Points)\n",
    "\n",
    "In the following, your task is to implement the K-Means clustering algorithm with K-Means++ initialization from scratch...well, almost from scratch. You can and should explore relevant methods provided by `numpy` or `sklearn`. The following subtasks will guide you step by step through the implementation, but you should also appreciate that K-Means is in it's core a very simple algorithm that can easily customized. We provide the skeleton code in the accompanying `.py` file as class `MyKMeans`.\n",
    "\n",
    "\n",
    "#### 2.1 a) Initializing Centroids Based on K-Means++ (5 Points)\n",
    "\n",
    "As we learned in the lecture, K-Means is rather sensitive to the initialization of the clusters. The most common initialization method is **K-Means++** (see lecture slides). Note that K-Means++ is non-deterministic as it picks the next centroids based on probabilities depending on the distances between the data points and the existing centroids. You will implement the K-Means++ initialization in this task.\n",
    "\n",
    "**Implement method `initialize_centroids()` to calculate the initial centroids based on K-Means++!** Note that we already imported the method `euclidean_distances()` for you, and you can use anything provided by `numpy` (*Hint:* Check out the method [`np.random.choice()`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) as it might  come in very handy).\n",
    "\n",
    "With the code cell below, you can test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beea7f1-0349-46e6-ae6c-edcd9423607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the randomness \"predictable\" so the result is always the same\n",
    "# (if you remove this line, the output will change for each run)\n",
    "np.random.seed(0) \n",
    "\n",
    "my_kmeans = MyKMeans(n_clusters=3)\n",
    "my_kmeans.initialize_centroids(X_kmeans)\n",
    "\n",
    "print(my_kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee87165-1c03-4ae1-95a3-0ff2b83752cd",
   "metadata": {},
   "source": [
    "The expected output of the code cell above is:\n",
    "    \n",
    "```\n",
    "[[-0.546 -8.162]\n",
    " [-7.801  5.454]\n",
    " [-4.856 -1.231]]\n",
    "\n",
    "```\n",
    "\n",
    "an array with $k=3$ rows -- one for each centroid (i.e., cluster center) -- and each row is an array with 2 coordinates since our dataset is just 2-dimensional. If you change the value for $k$, the shape of the array should change accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb6cffc-7217-4a47-9e93-eb77cbdbe4e4",
   "metadata": {},
   "source": [
    "#### 2.1 b) Assigning Data Points to Clusters (4 Points)\n",
    "\n",
    "In this step, each data point is assigned to its nearest centroid. Calculating distances and finding the smallest values is very easy with `sklearn` or `numpy`. (Hint: You may want to check [`np.argmin`](https://numpy.org/doc/stable/reference/generated/numpy.argmin.html))\n",
    "\n",
    "**Implement method `assign_clusters()` to assign each data point to its nearest centroid!** The method `assign_clusters()` needs to update `self.labels_` which is a 1-dimensional array of length $N$ (number of data points). Each element in `self.labels_` is a value ranging from $0$ to $(k-1)$ indicating to which cluster id a data point belongs to. For example, if `self.labels_ = [1, 0, 2, 2, 1]`, the first and the last data point belong to the same cluster. The second data point forms its own cluster; the third and forth data points form the third cluster (here $k=3$)\n",
    "\n",
    "With the code cell below, you can test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a86ccf-ecaf-455d-b9fd-4a69323e7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kmeans.assign_clusters(X_kmeans)\n",
    "\n",
    "print(my_kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32775aef-3af9-4586-8754-d8941c94e2c0",
   "metadata": {},
   "source": [
    "The expected output of the code cell above is:\n",
    "\n",
    "```\n",
    "[1 0 1 2 0 1 2 0 2 1 1 1 2 1 2 0 0 0 0 0 1 1 2 2 2 1 2 0 2 2 2 1 2 1 1 2 1\n",
    " 0 0 0 2 2 0 1 0 1 0 0 1 1 2 2 0 0 0 0 1 2 0 1 2 2 2 1 2 2 0 1 2 0 1 2 0 0\n",
    " 1 0 1 1 0 2 2 0 1 0 2 0 1 1 1 2 2 0 1 0 2 1 0 1 0 1]\n",
    "```\n",
    "\n",
    "Recall that we have 100 data points and set $k=3$. Hence, the length of the array is 100 and each element is either 0, 1, or 2, representing the cluster ids/labels. Note that the cluster id has no intrinsic meaning, it's only important that points in the same cluster have the same id/label. For example, the result `[1, 1, 0, 2, 1, 2]` would represent the same clustering as `[0, 0, 2, 1, 0, 1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb716c-4045-4f07-a074-395288f1a680",
   "metadata": {},
   "source": [
    "#### 2.1 c) Updating the Centroids (3 Points)\n",
    "\n",
    "After the assignment of the data points to clusters, all centroids need to be moved to the average of their respective clusters. Note that the centroids might not change because the assignment made no changes to the clusters and K-Means is done. We already handle this part in the given skeleton of the implementation.\n",
    "\n",
    "**Implement `update_centroids()` to update the centroids (i.e., cluster center) with respect to the new cluster assignments!** As usual, we recommend making good use of `numpy`, or maybe `sklearn`.\n",
    "\n",
    "With the code cell below, you can check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d12cf-d7e8-47bc-8180-a67b4c182cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kmeans.update_centroids(X_kmeans)\n",
    "\n",
    "print(my_kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2995cf05-7c3c-4451-9e79-9d38edcdd3f2",
   "metadata": {},
   "source": [
    "The expected output of the code cell above is:\n",
    "\n",
    "```\n",
    "[[ 4.81  -3.952]\n",
    " [-3.665  5.821]\n",
    " [-2.019  2.251]]\n",
    "```\n",
    "\n",
    "Again, the output reflects our setting of $k=3$ and our 2-dimensional dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5c739-7763-4713-9ccb-08b4e5b0be3b",
   "metadata": {},
   "source": [
    "#### Putting it all together (nothing for you to do here!)\n",
    "\n",
    "With the initialization of the centroids, the assignment to cluster, and the update of the centroids in place, we have everything in place to perform K-Means++ over a dataset X given a choice for k. The method `fit()` below performs K-Means++ using your implementations of the methods `initialize_centroids()`, `assign_clusters()`, and `update_centroids()`. Note that we can exit the main loop once there is no more change in the cluster assignments; if all fails, the algorithm stops after `max_iter` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a1d72f-6f3a-415c-8af2-971dd004b922",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5) \n",
    "\n",
    "my_kmeans = MyKMeans(n_clusters=3).fit(X_kmeans)\n",
    "\n",
    "print('Number of iterations: {}\\n'.format(my_kmeans.n_iter_))\n",
    "print('Final cluster centers:\\n{}\\n'.format(my_kmeans.cluster_centers_))\n",
    "print('Final cluster labels:\\n{}\\n'.format(my_kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f3380f-210a-4646-9ebc-7a723dc95a10",
   "metadata": {},
   "source": [
    "For a final test, you can also compare your implementation with scitkit-learn's implementation [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Since we adopted the variable and method names from `KMeans` for `MyKMeans`, the code in the cell below is almost identical to the one above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55988d30-c037-44a2-94e6-0ebf58c8281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5) \n",
    "\n",
    "sk_kmeans = KMeans(n_clusters=3, n_init=1).fit(X_kmeans)\n",
    "\n",
    "print('Number of iterations: {}\\n'.format(sk_kmeans.n_iter_))\n",
    "print('Final cluster centers:\\n{}\\n'.format(sk_kmeans.cluster_centers_))\n",
    "print('Final cluster labels:\\n{}\\n'.format(sk_kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c02a2-e2c8-43c3-bbf8-b416d7100faa",
   "metadata": {},
   "source": [
    "**Important:** As the initialization of the centroids has a random component, the result of `KMeans` and `MyKMeans` might generally not be the same. Even if the final centroids are the same, the order might differ. And again, recall from above that, e.g., the labels `[1, 1, 0, 2, 1, 2]` would represent the same clustering as `[0, 0, 2, 1, 0, 1]`. Only with the chosen values for `np.random.seed()` we see the same results. Still, note that the number of iterations differ.\n",
    "\n",
    "#### Visualization (nothing for you to do here) \n",
    "\n",
    "We provide you with auxiliary method `plot_kmeans_clustering()`; you can checkout `src.utils` to have a look at the code of the method. Feel free to go back and run `MyKMeans` or `KMeans` with different values for $k$ (visually we could already tell that $k=6$ is probably the best choice) and/or different random seeds (or not setting any seed at all). Just note that the given expected outcomes will no longer match for other values for $k$ and other seeds. But you can always visualize the result to assess whether the results look meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f17688-e386-41f6-b4cc-46710c1dd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_clustering(X_kmeans, my_kmeans.labels_, my_kmeans.cluster_centers_) # For your implementation\n",
    "#plot_kmeans_clustering(X_kmeans, my_kmeans.labels_, sk_kmeans.cluster_centers_) # For sklearn's implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb84749-da20-4e79-a091-512c9bd4733c",
   "metadata": {},
   "source": [
    "### 2.2 Questions about K-Means (6 Points)\n",
    "\n",
    "\n",
    "#### 2.2 a) Understanging K-Means++ Initialization (6 Points)\n",
    "\n",
    "The figure below shows six different datasets **A-F**, each containing 12 data points. The six datasets differ regarding their distribution of their data points.\n",
    "\n",
    "<img src=\"images/a1-kmeans-datasets.png\" />\n",
    "\n",
    "Now let's assume that we run K-Means over all six datasets with the following settings\n",
    "* Number of cluster: $k=3$\n",
    "* K-Means++ initialization\n",
    "* Data point 0 is forced to be the first centroid (this is the only change to the basic initilization)\n",
    "\n",
    "Since the K-Means++ initialization is randomized -- apart from the first centroid here! -- each data point has the chance to be picked as the location for Centroid 2 and Centroids 3. However, if we run the K-Means initialization multiple times (say, 1,000 times) for each dataset, we can check which data points are more likely to get picked as the location for Centroids 2 and 3. \n",
    "\n",
    "The following six collections of plots visualize this; one for each dataset. Each row of three plots reflect the distribution of frequencies how of a data point was chosen as Centroid 1 (1st plot), as Centroid 2 (2nd plot), and as Centroid 3 (3rd plot). Of course, since we force data point 1 always to be the location of Centroid 1, the first plot in each row has only a bar for data point 1.\n",
    "\n",
    "<img src=\"images/a1-kmeans-distribution-1.png\" />\n",
    "<img src=\"images/a1-kmeans-distribution-2.png\" />\n",
    "<img src=\"images/a1-kmeans-distribution-3.png\" />\n",
    "<img src=\"images/a1-kmeans-distribution-4.png\" />\n",
    "<img src=\"images/a1-kmeans-distribution-5.png\" />\n",
    "<img src=\"images/a1-kmeans-distribution-6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be35c73-0881-416d-b307-4363e0cf7080",
   "metadata": {},
   "source": [
    "Find the correct combinations of datasets and frequency distributions -- that is, find for each dataset the most likely frequency distribution! Give a brief explanation for each decision! Complete the table below! (The last line shows an example.)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce873ea0-460d-4cae-b54e-1fc11a04ffa6",
   "metadata": {},
   "source": [
    "| Dataset | Centroid Distribution | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | ??? | ??? |\n",
    "| **B**    | ??? | ??? |\n",
    "| **C**    | ??? | ??? |\n",
    "| **D**    | ??? | ??? |\n",
    "| **E**    | ??? | ??? |\n",
    "| **F**    | ??? | ??? |\n",
    "| **<font color=\"red\">X</font>**    | <font color=\"red\">9</font> | <font color='red'>The dataset plot looks like a heart, and the bars also form a heart shape (please come up with better explanations :) !)</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcb6f8-6978-4226-9fb6-fda52643f2d5",
   "metadata": {},
   "source": [
    "### 2.2 b) K-Means with Different Distance/Similarity metrics (2 Points)\n",
    "\n",
    "In the lecture, we used the Euclidean Distance between data points in centroids to perform the Lloyd's algorithm. However, there are other distance metrics such as the [Manhatten Distance](https://en.wikipedia.org/wiki/Taxicab_geometry) or the [Minkowski Distance](https://en.wikipedia.org/wiki/Minkowski_distance). Can we use those or similar metrics instead of the Euclidean Distance? Give a **brief** justification for your answer!\n",
    "\n",
    "**Your Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93c84b-a6b0-4805-8d16-860582f313db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c9692f-8ce4-4c2f-8a1f-2cfd03eb88d0",
   "metadata": {},
   "source": [
    "<img src=\"images/cs5228-header-title.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43445bb7",
   "metadata": {},
   "source": [
    "# Assignment 2 - Clustering & Association Rule Mining\n",
    "\n",
    "Hello everyone, this assignment notebook covers Clustering (again) and Association Rule Mining (ARM). There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A2_YourName_YourNUSNETID.ipynb** (e.g., **A2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Remember to rename and save the Python script file **A2_YourName_YourNUSNETID.py** (e.g., **A2_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is **Oct 3, 11.59 pm**. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if it's rather long; it's just important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6781ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_id = 'A0286188L'\n",
    "nusnet_id = 'E1237250'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b56a3",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well-indicated, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 DBSCAN & Comparing Cluster Algorithms (30 Points)**\n",
    "    * 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "        * 1.1 a) Compute Core Points (5 Points)\n",
    "        * 1.2 b) Compute Noise Points (5 Points)\n",
    "    * 1.2 Questions about Clustering Algorithms (20 Points)\n",
    "        * 1.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "        * 1.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "        * 1.2 c) Short Essay Questions (8 Points)\n",
    "* **2 Association Rule Mining (ARM) (20 Points)**\n",
    "    * 2.1 Implementing Apriori Algorithm (10 Points)\n",
    "        * 2.1 a) Create Candidate Itemsets $L_k$ (6 Points)\n",
    "        * 2.1 b) Generate Frequent Itemsets with Apriori Algorithm (4 Points)\n",
    "    * 2.2 Recommending Movies using ARM (10 Points)\n",
    "        * 2.2 a) Compare the Runs A-D and Discuss your Observations! (3 Points) \n",
    "        * 2.2 b) Compare the Runs A-D and Discuss the Results for Building a Recommendation Engine! (3 Points)\n",
    "        * 2.2 c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693531cc",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0483fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some magic so that the notebook will reload the external python script file any time you edit and save the .py file;\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5875aa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from efficient_apriori import apriori   # https://pypi.org/project/efficient-apriori/\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b583e6e9",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A2_YourName_YourNUSNETID.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dea6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from A2_VishwanathDattatreyaDoddamani_e1237250 import get_noise_dbscan\n",
    "#from A2_BobSmith_e12345678 import get_noise_dbscan # <-- you well need to rename this accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3339fa",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c3c04-fdad-4c4d-ace7-cf0f05df823f",
   "metadata": {},
   "source": [
    "## 1 DBSCAN & Comparing Cluster Algorithms (30 Points)\n",
    "\n",
    "\n",
    "### 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated below; the red dots in the next plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot below hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot below) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `A2.py` (before the appropriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) one containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 1.1 a) and 1.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 1.1 b) is important. This also means that you can ignore 1.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "We will benchmark your implementation as part of our Little Competitions to see whose solution is the fastest.\n",
    "\n",
    "#### Dataset Preparation (nothing for you to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87b9ec5-ce07-43fa-942a-ab640ba4a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_dbscan_toy is (70, 2)\n"
     ]
    }
   ],
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a2-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2afd7d-a291-4aa6-9ae5-691a0871d4fe",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85ef8907-3b79-441a-b669-38748f6d4490",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=10).fit(X_dbscan_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8456d-799a-480d-add1-5ab4df59f690",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1074f30e-0283-418c-8886-5d07cfcebce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices of the points labeled as noise are: [ 0  4 27 31 33 39 43 46 51 65]\n"
     ]
    }
   ],
   "source": [
    "cluster_point_indices = np.argwhere(dbscan_clustering.labels_ >= 0).squeeze()\n",
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e2f39-8a55-4aec-988f-6909b8f940df",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise as red triangles; all points belonging to *some* clusters are in grey points (note that we do not care to which exact cluster these points belong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58486bb2-8993-40a7-b0fc-7962acc2705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[cluster_point_indices,0], X_dbscan_toy[cluster_point_indices,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red', marker='^', s=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d72b4-fcc9-4576-9d9a-5737d452bbc0",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59bf9e6-0662-4dd8-b138-c287e2cae0ac",
   "metadata": {},
   "source": [
    "#### 1.1 a) Compute Core Points (5 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point belongs to, only that it is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163594f5-f327-4f26-8c8e-76ca798efd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of core points: 50\n",
      "\n",
      "The first 25 indices of the points labeled as core points:\n",
      "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441509bc-14f7-4c0e-bf41-944fd80f7c72",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` are missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise. Of course, also the border points are missing here, but [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) does not return those explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b27639-e3f0-4667-be2b-b87870c98334",
   "metadata": {},
   "source": [
    "#### 1.1 b) Compute Noise Points (5 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbea3a2c-84d5-4446-ad1b-e4610f52c87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of noise points: 10\n",
      "\n",
      "The indices of all points labeled as noise points:\n",
      "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n"
     ]
    }
   ],
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2989aeb-280c-402c-8123-b246cce1c479",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result matches the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82720640-0a91-4f47-aa5b-22f9372ce356",
   "metadata": {},
   "source": [
    "### 1.2 Questions about Clustering Algorithms (20 Points)\n",
    "\n",
    "#### 1.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering using AGNES (or any other hierarchical clustering technique). Properly interpreting them is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions.\n",
    "\n",
    "<img src=\"images/a2-agnes-data-labeled.png\">\n",
    "\n",
    "Below are 6 dendrograms labeled 1-6. These dendograms show the clustering using AGNES with Single Linkage for the 6 datasets above, but in a random order.\n",
    "\n",
    "<img src=\"images/a2-agnes-dendrogram-labeled.png\">\n",
    "\n",
    "Find the correct combinations of datasets and dendrograms -- that is, find for each dataset the dendrogram that visualizes the clustering using AGNES with Single Linkage! Give brief explanation for each decision! Complete the table below! (The last line shows an example.)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f647bb9-7a94-437b-b62f-ed6bbc4b7f1c",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram | Brief Explanation |\n",
    "| ---  | ---   | ---                  |\n",
    "| **A**    | 6 | There are two clusters that can be seen of equal size which is reflected in 6th dendrogram. |\n",
    "| **B**    | 5 | The center point of the circle will be merged last, while all the other data points merge before, this can be seen in 5th dendrogram. |\n",
    "| **C**    | 2 | Two points are far from rest of the data points and hence should be merged last. This sequence is noticeable in 2nd dendrogram. |\n",
    "| **D**    | 4 | There are three well separated clusters of somewhat equal size that can be seen immediately which is noticed in 4th dendrogram. |\n",
    "| **E**    | 1 | The random data points merging pattern can be seen in 1st dendrogram. |\n",
    "| **F**    | 3 | The distance between data points is increasing, which means that each data point will join the cluster at each level. This exact pattern can be seen in 3rd dendrogram. |\n",
    "| **<font color='red'>X</font>**    | **<font color='red'>9</font>** | <font color='red'>The dataset plot looks like a face and the dendrogram looks like a hat (please come up with better explanations :) !)</font> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91166-3520-43ed-8292-ea99890bec92",
   "metadata": {},
   "source": [
    "#### 1.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "\n",
    "The figure belows shows the 6 different clusterings A-F, each computed over a dataset of 8 unique data points $x_1 x_2, ..., x_8$. The datasets are independent from each other for the 6 clusterings. Each clustering contains 3 clusters are represented by the table. A `1` in the result table indicates that the corresponding data point is part of the corresponding cluster. For example, in Clustering A, the `1` in the bottom-left cell indicates that data point $x_8$ is part of Cluster $C_1$.\n",
    "\n",
    "**Addtional constraints:**\n",
    "\n",
    "* For K-Means and DBSCAN, the 3 cluster $C_1$, $C_2$, and $C_3$ are the **only** clusters; for AGNES you can assume there might be **more** clusters in the hierarchy\n",
    "* For DBSCAN, the input parameter for the minimum number of neighboring points is  $MinPts \\geq 2$\n",
    "\n",
    "<img src=\"images/a2-clustering-comparison.png\">\n",
    "\n",
    "**For each clustering, decide which algorithm (K-Means, DBSCAN, AGNES) can have produced the clustering!** Use the table below for the answer. If an algorithm could have produced a clustering, just write *OK* in the respective cell of the table. If an algorithm could not have produced a clustering, enter a brief explanation into the respective table cell.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc323e0-ac0c-488c-9bc0-8c57e56cd370",
   "metadata": {},
   "source": [
    "|  | K-Means | DBSCAN       | AGNES |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------| ------- |\n",
    "| **Clustering A**  | OK | There cannot be empty clusters in DBSCAN | There cannot be empty cluster in AGNES |\n",
    "| **Clustering B**  | K-Means is exclusive and in clustering B there are some data points that belong to more than one cluster | DBSCAN is exclusive and in clustering B there are some data points that belong to more than one cluster | OK |\n",
    "| **Clustering C**  | There is no exclusivity and completeness | There is no exclusivity | There is no completeness |\n",
    "| **Clustering D**  | There is no completeness | OK | There is no completeness |\n",
    "| **Clustering E**  | OK | Min 2 points are required to form a cluster, C2 only has one data point | OK |\n",
    "| **Clustering F**  | There is no exclusivity | There is no exclusivity | OK |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb4d6c-5ba6-415b-8efa-9e62618f0441",
   "metadata": {},
   "source": [
    "#### 1.2 c) Short Essay Questions (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b688752-fb83-4542-bc15-fe41794a3fb1",
   "metadata": {},
   "source": [
    "Recall the K-Means results in clusterings that are complete, i.e., each data point is assigned to a cluster. In contrast, DBSCAN has the notion of noise, i.e., points that are not part of any cluster, which can be used to identify outliers (see also Task 1.1). Now let's assume we want to identify outliers in a dataset but only have a standard implementation for K-Means available.\n",
    "\n",
    "**How can we utilize K-Means to (potentially) identify outliers? (2 Points)** Since the notion of outliers is not well defined, its not about having a fool-proof solution but to make a well-informed decision to limit the set of data points that are potential outliers.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ea51b-2e1d-4b49-9ecf-6dce70b63126",
   "metadata": {},
   "source": [
    "1) Run K-Means and assign all data points to clusters and get standard deviation of each cluster along with centroid (mean).\n",
    "2) Calculate z-score of all data points in all clusters.\n",
    "3) If z-score is greater than 3 or less than -3, then mark those data points as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20963e-f0e5-45d4-8ca8-b9c37c31fab1",
   "metadata": {},
   "source": [
    "We saw in the lecture that K-Means can return empty clusters.\n",
    "\n",
    "**In which situation may K-Means return at least 1 empty cluster? (2 Points)** To address this task, please make the following assumptions:\n",
    "* The number of data points $N$ in the dataset is $N > 0$\n",
    "* The number of clusters $K$ when running K-Means is $K \\geq 2$\n",
    "* The initialization of the initial centroids can be arbitrarily good or bad\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a44b-600f-4254-a771-5f598a0e77ea",
   "metadata": {},
   "source": [
    "1) If K > N, then K-Means returns empty clusters.\n",
    "2) If the initialization of centroids is bad then a centroid is far from data points making its cluster empty (centroid is 'blocked off' data points by other centroids). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91a8af-651f-43f3-aef1-5f170bd345cb",
   "metadata": {},
   "source": [
    "Assume your dataset contains the geolocations of traffic accidents on Singapore expressways over the time span of a year. Using AGNES, you want to find sections of the expressways where traffic accidents are particularly common.\n",
    "\n",
    "**Which Linkage Methods covered in the lecture is most suitable for this task? (2 Points)** Briefly explain your choice!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f1231-0d3e-41fa-a895-0ad5e1c41fcb",
   "metadata": {},
   "source": [
    "1) Complete linkage method is suitable for this task as it clusters data points based on farthest distance, making it a good choice in identifying a the minimum area of an accident section in the expressway and it is also less susceptible to noise (Single linkage might form a large cluster size or large area of a section since it considers nearest distance and thereby diluting the cluster or chaining).\n",
    "2) Average linkage method is also suitable for this task since it is not prone to globular clusters and hence can spot the section with lesser area compared to single linkage.\n",
    "3) Ward linkage method is most suitable for this task as it identifies specific and well-separated clusters better than the rest of the linkage methods. As it minimizes variance within clusters, it helps identify dense clusters that are accident hotspots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0fd71-2852-4be9-8749-37e7d7688a28",
   "metadata": {},
   "source": [
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X` (i.e., randomly change the order of data points in the dataset); no other changes. Now you run DBSCAN with the *same* parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**What does this information tell about the dataset and clustering? (2 Points)** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679e54d-9e37-40bb-98c4-5aab965a08a1",
   "metadata": {},
   "source": [
    "1) Since the clusters are not exactly same as before, it tells us that dataset contains a lot of border points.\n",
    "2) If eps is increased or min points is decreased then the result might be more deterministic even after shuffling as this increases more core points (core points are deterministic, border and noise points aren't deterministic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c755cb-3bc4-4428-b675-ed97bc22836d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec0879",
   "metadata": {},
   "source": [
    "## 2 Association Rule Mining (ARM)\n",
    "\n",
    "Your task is to implement the Apriori Algorithm for finding Association Rules. In more detail, we focus on the **Apriori Algorithm for finding Frequent Itemsets** -- once we have the Frequent Itemsets, we use a naive approach for the association rule. We will provide a small method for that part later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6af0f",
   "metadata": {},
   "source": [
    "### 2.1 Implementing Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c62cd6",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transactions and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6b311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3ad90",
   "metadata": {},
   "source": [
    "#### Auxiliary Methods\n",
    "\n",
    "We want you to focus on the Apriori algorithm. So we provide a set of auxiliary functions. Feel free to look at their implementation in the file `src/utils.py`.\n",
    "\n",
    "The method `unique_items()` returns all the unique items across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72077b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bread', 'cereal', 'cheese', 'eggs', 'milk', 'yogurt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_items(transactions_demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629cf522",
   "metadata": {},
   "source": [
    "The method `support()` calculates and returns the support for a given itemset and set of transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d86aeb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support(transactions_demo, ('bread', 'milk'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0fce82",
   "metadata": {},
   "source": [
    "The method `confidence()` calculates and returns the confidence for a given association rule and set of transactions. An association rule is represented by a 2-tuple, where the first element represents itemset X and the second element represents items Y (i.e., $X \\Rightarrow Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fa0e07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence(transactions_demo, (('bread',), ('milk',)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49823188",
   "metadata": {},
   "source": [
    "The method `merge_itemsets()` merges two given itemsets into one itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4cf8e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bread', 'eggs', 'milk')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_itemsets(('bread', 'milk'), ('bread', 'eggs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667ae69",
   "metadata": {},
   "source": [
    "For your implementation, you can make use of these auxiliary methods wherever you see fit. And that is, of course, strongly recommended, as it makes the programming task much easier. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49dbef4",
   "metadata": {},
   "source": [
    "#### 2.1 a) Create Candidate Itemsets $L_k$ (6 Points)\n",
    "\n",
    "Let's assume we have found $F_{k-1}$, i.e., all Frequent Itemsets for size $k-1$. For example $F_1$ is the set of all Frequent Itemsets of size 1, which is simply the set of unique items across all transactions with sufficient support. The next step is now to find $L_k$, all Candidate Itemsets of size $k$. In the lecture, we introduced two methods for this. For this assignment, we focus on the $\\mathbf{F_{k-1} \\times F_{k-1}}$ method -- that is, we use the Frequent Itemsets from the last step to calculate the Candidate Itemsets for the current step.\n",
    "\n",
    "Recall from the lecture that creating $L_k$ involves two main parts:\n",
    "\n",
    "* **Generating** all possible $k$-itemsets from the Frequent Itemsets $F_{k-1}$; and\n",
    "\n",
    "* **Pruning** all $k$-itemsets that cannot be frequent based on the information we already have ($L_k$ should only contain the itemsets for which we indeed calculate the support for)\n",
    "\n",
    "\n",
    "Recall that we also can (and should) **prune** any Candidate Itemsets than cannot possibly also be Frequent Itemsets  based on the information we already have. In other words, the Candidate Itemsets of size $k$ should only contain the itemsets for which we indeed calculate the support for.\n",
    "\n",
    "**Hint:** In the lecture, to make it more illustrative, we first generate all possible Candidate Itemsets and then prune the ones that cannot possibly be frequent. In practice, to save memory space, it's better to check each Candidate Itemset immediately before even adding it to $L_k$. The skeleton code below reflects this. However, if you indeed want to implement pruning as its own step, you're free to do so.\n",
    "\n",
    "**Implement method `generate_Lk()` to calculate the Candidate Itemsets $L_k$ given the Frequent Itemsets $F_{k-1}$!** Note that we walked in detail through an example of this process in the lecture. Below is a code cell that reflects this example to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6413261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Lk(Fk_minus_one):\n",
    "\n",
    "    # The code just looks a bit odd since we cannot get an element from a set using indexing\n",
    "    k = len(next(iter(Fk_minus_one))) + 1\n",
    "\n",
    "    # Initialize as set as a fail safe to avoid any duplicates\n",
    "    Lk = set()\n",
    "    \n",
    "    for itemset1 in Fk_minus_one:\n",
    "        for itemset2 in Fk_minus_one:\n",
    "            \n",
    "            ######################################################################\n",
    "            ### Your code starts here ############################################\n",
    "            itemset = merge_itemsets(itemset1, itemset2)\n",
    "            if len(itemset) == k:\n",
    "                gen_powerset = powerset(itemset, min_len=k-1, max_len=k-1)\n",
    "                flag = 0\n",
    "                for gen_itemset in gen_powerset:\n",
    "                    if gen_itemset not in Fk_minus_one:\n",
    "                        flag = 1\n",
    "                if flag == 0:\n",
    "                    Lk.add(itemset)\n",
    "                    \n",
    "            ### Your code ends here ##############################################\n",
    "            ######################################################################\n",
    "            \n",
    "            pass # Just there so the empty loop does not throw an error\n",
    "    \n",
    "    ######################################################################\n",
    "    ### Your code starts here ############################################\n",
    "    \n",
    "    # MAY ONLY BE REQUIRED IF YOU TREAT PRUNING AS A SEPARATE STEP!!!\n",
    "    # which you shouldn't for performance reasons in practice, but for the assignment its fine\n",
    "    \n",
    "    ### Your code ends here ##############################################\n",
    "    ######################################################################\n",
    "    \n",
    "    return Lk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45e92ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bread', 'milk', 'yogurt')\n",
      "('cheese', 'milk', 'yogurt')\n",
      "('cereal', 'milk', 'yogurt')\n",
      "('bread', 'cereal', 'yogurt')\n",
      "('bread', 'cereal', 'milk')\n"
     ]
    }
   ],
   "source": [
    "k_itemsets = generate_Lk({\n",
    "    ('bread', 'cereal'), ('bread', 'milk'), ('bread', 'yogurt'), ('cereal', 'milk'),\n",
    "    ('cereal', 'yogurt'), ('cheese', 'milk'), ('cheese', 'yogurt'), ('milk', 'yogurt')\n",
    "})\n",
    "\n",
    "for itemset in k_itemsets:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7ce928",
   "metadata": {},
   "source": [
    "#### 2.1 b) Generate Frequent Itemsets with Apriori Algorithm (4 Points)\n",
    "\n",
    "The method `generate_Lk()` covered the \"Generate\" and \"Prune\" steps of the Apriori Algorithm for finding Frequent Itemsets. Now only the \"Calculate\" and \"Filter\" step is missing. However, with `generate_Lk()` in place and together with the auxiliary methods we provide (see above), putting the Apriori Algorithm together should be pretty straightforward.\n",
    "\n",
    "**Implement `frequent_itemsets_apriori()` to find all Frequent Itemset given a set of transactions and a minimum support of `min_support`!** Again, below is a code cell that reflects this example to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63b0a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_itemsets_apriori(transactions, min_support):\n",
    "    \n",
    "    # The frequent 1-itemsets are all unique items across all transactions with sufficient support\n",
    "    # The one-liner below simply loops over all uniques items and checks the condition w.r.t. the support\n",
    "    F1 = set([(s,) for s in unique_items(transactions) if support(transactions, (s,)) >= min_support ])\n",
    "    \n",
    "    # If there is not even a single 1-itemset that is frequent, we can just stop here\n",
    "    if len(F1) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize dictionary with all current frequent itemsets for each size k\n",
    "    # Example: { 1: {(a), (b), (c)}, 2: {(a, c), ...} }\n",
    "    F = { 1: F1 }\n",
    "    \n",
    "    # Find now all frequent itemsets of size 2, 3, 4, ... (sys.maxsize basically mean infinity here)\n",
    "    for k in range(2, sys.maxsize):\n",
    "\n",
    "        Fk = set()\n",
    "        \n",
    "        ########################################################################################\n",
    "        ### Your code starts here ##############################################################\n",
    "        candidate_k = generate_Lk(F[k-1])\n",
    "        Fk = set([s for s in candidate_k if support(transactions, s) >= min_support])\n",
    "        if len(Fk) == 0:\n",
    "            break\n",
    "        ### Your code ends here ################################################################\n",
    "        ########################################################################################\n",
    "                \n",
    "        F[k] = Fk    \n",
    "\n",
    "    # Merge the dictionary of itemsets to a single set and return it\n",
    "    # Example: {1: {(a), (b), (c)}, 2: (a, c)} => {(a), (b), (c), (a, c)}\n",
    "    return set.union(*[ itemsets for k, itemsets in F.items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ce7be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('milk',)\n",
      "('cereal', 'milk')\n",
      "('bread',)\n",
      "('bread', 'yogurt')\n",
      "('yogurt',)\n",
      "('cereal',)\n",
      "('bread', 'milk')\n",
      "('milk', 'yogurt')\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets = frequent_itemsets_apriori(transactions_demo, 0.6)\n",
    "for itemset in frequent_itemsets:\n",
    "    print(itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d0094",
   "metadata": {},
   "source": [
    "#### From Frequent Itemsets to Association Rules (nothing for you to do here!)\n",
    "\n",
    "Your implementation so far gives you the Frequent Itemsets in a list of transactions using the Apriori method. This step is typically the most time-consuming one in Association Rule Mining. However, we still have to do the second step and find all Association Rules given the Frequent Itemsets. We saw in the lecture that this can also be done in an efficient manner using the Apriori method to avoid checking all rules.\n",
    "\n",
    "Since this step is typically less computationally expensive, we simply do it the naive way -- that is, we go over all Frequent Itemsets, and check for each Frequent Itemset and which of the Association Rules that can be generated from it has a sufficiently high confidence. With all the auxiliary methods we provide, this becomes trivial to implement, so we simply give you the method `find_association_rules()` below. Note how it uses your implementation of `frequent_itemsets_apriori()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6abc8144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_association_rules(transactions, min_support, min_confidence):\n",
    "    # Initialize empty list of association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    # Find and loop over all frequent itemsets\n",
    "    for itemset in frequent_itemsets_apriori(transactions, min_support):\n",
    "        if len(itemset) == 1:\n",
    "            continue\n",
    "\n",
    "        # Find and loop over all association rules that can be generated from the itemset\n",
    "        for r in generate_association_rules(itemset):\n",
    "            # Check if the association rule fulfils the confidence requriement\n",
    "            if confidence(transactions, r) >= min_confidence:\n",
    "                association_rules.append(r)\n",
    "                \n",
    "    # Return final list of association rules\n",
    "    return association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2ba7b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('cereal',), ('milk',))\n",
      "(('cheese',), ('milk', 'yogurt'))\n",
      "(('cheese', 'milk'), ('yogurt',))\n",
      "(('cheese', 'yogurt'), ('milk',))\n",
      "(('cheese',), ('yogurt',))\n",
      "(('cereal', 'yogurt'), ('milk',))\n",
      "(('bread', 'cereal'), ('milk',))\n",
      "(('cheese',), ('milk',))\n"
     ]
    }
   ],
   "source": [
    "for rule in find_association_rules(transactions_demo, 0.4, 1.0):\n",
    "    print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88167699",
   "metadata": {},
   "source": [
    "#### Comparison with `efficient-apriori` package  (nothing for you to do here!)\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `min_support` and `min_confidence` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62eee91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('bread', 'cereal') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cereal', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'milk') => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk', 'yogurt')] (support: 0.4, confidence: 1.0, lift: 1.6666666666666667)\n"
     ]
    }
   ],
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.4, min_confidence=1.0)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70dbc",
   "metadata": {},
   "source": [
    "The `efficient-apriori` provides, of course, a much more efficient and convenient implementation (e.g., keeping track of all the metrics for each rule). And this is why we use this package for finding Association Rules in a real-world dataset below. Still, in its core, `efficient-apriori` implements the same underlying Apriori method to Find Frequent Itemsets (but also to find the Association Rules). If you're interested, further below, you can compare the runtimes of `efficient-apriori` and your implementation. Just don't be too disappointed :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443ffda",
   "metadata": {},
   "source": [
    "### 2.2 Recommending Movies using ARM\n",
    "\n",
    "In this task, we look into using Association Rule Mining for recommending movies -- more specifically, recommending movies on physical mediums (Blu-ray, DVD, etc.), assuming that is still a thing nowadays :).\n",
    "\n",
    "**Dataset.** E-commerce sites do not really make their data publicly available, so we do not have any hard real-world dataset. For the context of this assignment, this is of course no problem. What we use here is a popular movie ratings dataset from [MovieLens](https://grouplens.org/datasets/movielens/). This dataset contains user ratings for movies (1-5 stars, incl. half stars, e.g., 3.5). More specifically, we use the [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/) containing 1 Million ratings from ~6,000 users on ~4,000 movies and was released February 2003 -- so do not expect any recent Marvel movies :).\n",
    "\n",
    "While using these ratings allow for more sophisticated recommendation algorithms -- and we will look into some of those in a later lecture -- here we are focusing on Association Rules. This includes that we need to convert this rating dataset into a transaction dataset, where a transaction represents all the movies a user has purchased. We already did this for you making the following assumption: A User has purchased all the movies s/he gave the highest rating. For example, if User A gave a highest rating of 4.5 to any movie, A has purchased all movies A rated with 4.5. This is certainly a simplifying assumption, but perfectly fine for this task here.\n",
    "\n",
    "Let's have a quick look at the data. First, we load the ids and names of all movies into a dictionary. We need this dictionary since our transactions (i.e., the list of movies a user has bought) contains the ids and not the names of the movies. So to actually see the names of movies in the association rules, we need this way to map from a movie's id to its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d934b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> Toy Story\n",
      "2 -> Jumanji\n",
      "3 -> Grumpier Old Men\n",
      "4 -> Waiting to Exhale\n",
      "5 -> Father of the Bride Part II\n"
     ]
    }
   ],
   "source": [
    "# Read file with movies (and der ids) into a pandas dataframe\n",
    "df_movies = pd.read_csv('data/a2-arm-movies.csv', header=None)\n",
    "# Convert dataframe to dictionary for quick lookups\n",
    "movie_map = dict(zip(df_movies[0], df_movies[1]))\n",
    "# Show the first 5 entries as example\n",
    "for movie_id, movie_name in movie_map.items():\n",
    "    print('{} -> {}'.format(movie_id, movie_name))\n",
    "    if movie_id >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e553756",
   "metadata": {},
   "source": [
    "No we can load the transactions. Again, a transaction is a user's shopping history, i.e., all the movies the user has bought. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3717ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopping history for user 0 (used for Aprior algorithm)\n",
      "(1, 48, 150, 527, 595, 1022, 1028, 1029, 1035, 1193, 1270, 1287, 1836, 1961, 2028, 2355, 2804, 3105)\n",
      "\n",
      "Detailed shopping history for user 0\n",
      "1: Toy Story\n",
      "48: Pocahontas\n",
      "150: Apollo 13\n",
      "527: Schindler's List\n",
      "595: Beauty and the Beast\n",
      "1022: Cinderella\n",
      "1028: Mary Poppins\n",
      "1029: Dumbo\n",
      "1035: Sound of Music, The\n",
      "1193: One Flew Over the Cuckoo's Nest\n",
      "1270: Back to the Future\n",
      "1287: Ben\n",
      "1836: Last Days of Disco, The\n",
      "1961: Rain Man\n",
      "2028: Saving Private Ryan\n",
      "2355: Bug's Life, A\n",
      "2804: Christmas Story, A\n",
      "3105: Awakenings\n"
     ]
    }
   ],
   "source": [
    "shopping_histories = []\n",
    "\n",
    "# Read shopping histories; each line is a comma-separated list of the movies (i.e., their ids!) a user bought\n",
    "with open('data/a2-arm-movie-shopping-histories.csv') as file:\n",
    "    for line in file:\n",
    "        shopping_histories.append(tuple([ int(i) for i in line.strip().split(',') ]))\n",
    "\n",
    "# Show the shopping history of the first user for an example; we need movie_map to get the name of each movie\n",
    "user = 0\n",
    "\n",
    "print('Shopping history for user {} (used for Aprior algorithm)'.format(user))\n",
    "print(shopping_histories[user])\n",
    "print()\n",
    "print('Detailed shopping history for user {}'.format(user))\n",
    "for movie_id in shopping_histories[user]:\n",
    "    print('{}: {}'.format(movie_id, movie_map[movie_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79366a78",
   "metadata": {},
   "source": [
    "With the dataset loaded, we are ready to find interesting Association Rules. For performance reasons, we use the `efficient_apriori` package -- however, further below there is an optional code cell where you can use your own implementation of the Apriori algorithm, in case you are interested.\n",
    "\n",
    "For added convenience, we provide method `show_top_rules()` which computes the Association Rules using the `efficient-apriori` package, but (a) sorts the rules w.r.t. the specified metric (default: lift), and (b) shows only the top-k rules (default: 5). The method also ensures a consistent output of each Association Rule. Each rule contains the LHS, RHS, as well as the support (s), confidence (c), and lift (l). Feel free to check out the code of method `show_top_rules()` in `src.utils` if anything might be unclear regarding its use.\n",
    "\n",
    "**Run the following 4 code cells and interpret the results below!** All 4 code cells find Association Rules using the `efficient-apriori` package encapsulated in the auxiliary method `show_top_rules()` for convenience. Appreciate how Runs A-B differ with respect to the input parameter of the method calls! Also, note that we call `show_top_rules()` with `id_map=None` at first, so the results will only display the movie ids. Later, you will be asked to run the cells again with `id_map=movie_map` to see the actual names of the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2d64405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 68 ===\n",
      "(Godfather: Part II, The) => (Godfather, The)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(Godfather, The) => (Godfather: Part II, The)  [s: 0.13, c: 0.55, l: 3.51]\n",
      "(Star Wars: Episode IV; Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode IV; Star Wars: Episode VI)  [s: 0.11, c: 0.44, l: 3.43]\n",
      "(Star Wars: Episode IV; Star Wars: Episode V) => (Star Wars: Episode VI)  [s: 0.11, c: 0.58, l: 3.42]\n",
      "(Star Wars: Episode VI) => (Star Wars: Episode IV; Star Wars: Episode V)  [s: 0.11, c: 0.63, l: 3.42]\n",
      "(Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.12, c: 0.73, l: 2.96]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode VI)  [s: 0.12, c: 0.51, l: 2.96]\n",
      "(Star Wars: Episode IV; Raiders of the Lost Ark) => (Star Wars: Episode V)  [s: 0.11, c: 0.71, l: 2.88]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode IV; Raiders of the Lost Ark)  [s: 0.11, c: 0.46, l: 2.88]\n",
      "(Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "\n",
      "CPU times: user 63.8 ms, sys: 5.17 ms, total: 68.9 ms\n",
      "Wall time: 72.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run A\n",
    "# show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.2, k=10, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b75ea14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 2962203 ===\n",
      "(Manon of the Spring) => (Jean de Florette)  [s: 0.01, c: 0.64, l: 35.18]\n",
      "(Jean de Florette) => (Manon of the Spring)  [s: 0.01, c: 0.60, l: 35.18]\n",
      "(Wrong Trousers, The; Godfather: Part II, The) => (Close Shave, A; Godfather, The)  [s: 0.01, c: 0.60, l: 28.40]\n",
      "(Close Shave, A; Godfather, The) => (Wrong Trousers, The; Godfather: Part II, The)  [s: 0.01, c: 0.48, l: 28.40]\n",
      "(Star Wars: Episode IV; Wrong Trousers, The; Star Wars: Episode V) => (Close Shave, A; Star Wars: Episode VI)  [s: 0.01, c: 0.39, l: 27.32]\n",
      "(Close Shave, A; Star Wars: Episode VI) => (Star Wars: Episode IV; Wrong Trousers, The; Star Wars: Episode V)  [s: 0.01, c: 0.70, l: 27.32]\n",
      "(Star Wars: Episode IV; Close Shave, A; Star Wars: Episode V) => (Wrong Trousers, The; Star Wars: Episode VI)  [s: 0.01, c: 0.49, l: 27.29]\n",
      "(Wrong Trousers, The; Star Wars: Episode VI) => (Star Wars: Episode IV; Close Shave, A; Star Wars: Episode V)  [s: 0.01, c: 0.56, l: 27.29]\n",
      "(Godfather, The; Wrong Trousers, The) => (Close Shave, A; Godfather: Part II, The)  [s: 0.01, c: 0.37, l: 26.54]\n",
      "(Close Shave, A; Godfather: Part II, The) => (Godfather, The; Wrong Trousers, The)  [s: 0.01, c: 0.73, l: 26.54]\n",
      "(Star Wars: Episode IV; Close Shave, A; Wrong Trousers, The) => (Star Wars: Episode V; Grand Day Out, A)  [s: 0.01, c: 0.39, l: 25.65]\n",
      "\n",
      "CPU times: user 47.1 s, sys: 4.51 s, total: 51.6 s\n",
      "Wall time: 54.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run B\n",
    "# show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.2, k=10, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.2, k=10, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "563ec2f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 4 ===\n",
      "(Godfather: Part II, The) => (Godfather, The)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(Star Wars: Episode IV; Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "(Star Wars: Episode V; Raiders of the Lost Ark) => (Star Wars: Episode IV)  [s: 0.11, c: 0.84, l: 2.76]\n",
      "\n",
      "CPU times: user 47.1 ms, sys: 3.93 ms, total: 51 ms\n",
      "Wall time: 50.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run C\n",
    "# show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee231d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 122486 ===\n",
      "(Close Shave, A; Star Wars: Episode V; Grand Day Out, A) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.84, l: 21.17]\n",
      "(Close Shave, A; Star Wars: Episode V; Raiders of the Lost Ark) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.83, l: 20.89]\n",
      "(Close Shave, A; Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.81, l: 20.55]\n",
      "(Wallace & Gromit: The Best of Aardman Animation; Grand Day Out, A) => (Close Shave, A; Wrong Trousers, The)  [s: 0.01, c: 0.82, l: 15.70]\n",
      "(Matrix, The; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.83, l: 15.44]\n",
      "(Toy Story; Grand Day Out, A) => (Close Shave, A; Wrong Trousers, The)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(Reservoir Dogs; Star Wars: Episode V; Godfather: Part II, The; American Beauty) => (Star Wars: Episode IV; Pulp Fiction; Godfather, The)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(Reservoir Dogs; Star Wars: Episode V; Raiders of the Lost Ark; Godfather: Part II, The) => (Star Wars: Episode IV; Pulp Fiction; Godfather, The)  [s: 0.01, c: 0.80, l: 15.34]\n",
      "(Star Wars: Episode IV; Star Wars: Episode V; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.82, l: 15.27]\n",
      "(Godfather, The; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.81, l: 15.07]\n",
      "(Star Wars: Episode V; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.81, l: 14.96]\n",
      "\n",
      "CPU times: user 36.8 s, sys: 1.27 s, total: 38 s\n",
      "Wall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run D\n",
    "# show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b376c706",
   "metadata": {},
   "source": [
    "**Optional:** Feel free to uncomment and run the code cell below. It uses your implementation of the Apriori algorithm using the same parameters as Run C. You can use this code to double-check your implementation, but please be aware that it will run longer than the `efficient_apriori` package; although not too long for these parameters. Note that the result will not be in the same format and not sorted, but you can easily eyeball that the results will match the one of Run C above...or at least should :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0565a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1221,) => (858,)\n",
      "(1196, 1198) => (260,)\n",
      "(260, 1210) => (1196,)\n",
      "(1196, 1210) => (260,)\n",
      "CPU times: user 26.1 s, sys: 119 ms, total: 26.3 s\n",
      "Wall time: 26.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rules = find_association_rules(shopping_histories, 0.1, 0.8)\n",
    "\n",
    "for lhs, rhs in rules:\n",
    "   print('{} => {}'.format(lhs, rhs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc0961",
   "metadata": {},
   "source": [
    "#### 2.2 a) Compare the Runs A-D and Discuss your Observations! (3 Points)\n",
    "\n",
    "You must have noticed numerous differences between the 4 runs A-D. List at least 3 differences you have found. You may want to consider the elapsed time and the resulting association rules. Briefly explain your observations! For this subtask, you do not need to look at the movie names (`id_map=None`) as your observations are not specific to the context of movie recommendations; at this we will look in 2.2 b)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc66569",
   "metadata": {},
   "source": [
    "1) If there are more association rules, then the elapsed time is more (Runs B and D take significantly more time than runs A and C but B and D have significantly more rules compared to A and C).\n",
    "2) If min_support is less then that results in more rules as there is less pruning leading to more compute time.\n",
    "3) If min_confidence is less then that also results in more rules. However, there is not a large order of magnitude of difference when min_confidence is increased when compared with increasing min_support. This means that min_confidence helps in reducing some rules but does not give huge advantage in terms of compute time.\n",
    "4) More rules also result in frequent itemsets with higher k compared to the results of less rules.\n",
    "5) If min_support is high and min_confidence is low, then less rules and less compute time.\n",
    "6) If min_support is low and min_confidence is low, then highest number of rules leading to highest compute time.\n",
    "7) If min_support is high and min_confidence is high, then lowest number of rules leading to lowest compute time.\n",
    "8) If min_support is low and min_confidence is high, then more rules leading to more compute time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bc62c",
   "metadata": {},
   "source": [
    "Now run the code cells above for Runs A-D again, but this time with `id_map=movie_map` so that the output will show for each rule the actual movie names.\n",
    "\n",
    "#### 2.2 b) Compare the Runs A-D and discuss the results for building a recommendation engine! (3 Points)\n",
    "\n",
    "Comparing the results of the different runs again, but now seeing the actual movie names, should give you some further insights how the choice of `min_support` and `min_confidence` might affect how the resulting rules are useful for building a recommendation engine.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e0931",
   "metadata": {},
   "source": [
    "1) If min_support is high and min_confidence is low, the movies in (antecedent union consequent) frequently appear together. If the movies in antecedent appear together, they often do so without the movies in consequent.\n",
    "2) If min_support is low and min_confidence is low, the movies in (antecedent union consequent) do not frequently appear together. Even if the movies in antecedent appear together, they do so often without the movies in consequent.\n",
    "3) If min_support is high and min_confidence is high, the movies in (antecedent union consequent) frequently appear together. If the movies in antecedent appear together, they do so often with the movies in consequent.\n",
    "4) If min_support is low and min_confidence is high, the movies in (antecedent union consequent) do not frequently appear together. If the movies in antecedent appear together, they often do so with the movies in consequent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b84665",
   "metadata": {},
   "source": [
    "#### 2.2 c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)\n",
    "\n",
    "So far, we only looked at individual rules and how the set of rules changes for different parameter values for `min_support` and `min_confidence`. However, we still need some method like `make_recommendation(shopping_history)` that takes the shopping history of a user and returns 1 or more recommendations. The goal is here is *not* to implement such a method but outline the main concerns to consider when implementing such a method\n",
    "\n",
    "\n",
    "(Hint: Do not forget that you not only have the information about Association Rules but also about the individual Frequent Itemsets)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df19f8b",
   "metadata": {},
   "source": [
    "<p><b>Algorithm Sketch: </b>Take shopping cart data of all users and generate frequent itemsets. Then generate association rules using min_support and min_confidence using ARM. That is, 2.2 is implemented. Then based on user's shopping_history, antecedent rules are filtered, i.e., if a user has purchased all the movies of the antecedent rule, then based on consequent, the movie/s is/are recommended. However, there are quite a few concerns that need to carefully considered. These potential issues and brief solution to those problems are outlined below. </p>\n",
    "<p><b>1) New User: </b> If the user is new then there is no shopping history causing a concern. In this case, recommendation can be done based on the most popular movies.</p>\n",
    "<p><b>2) Lack of Personalization: </b> The recommendation is based on what other people had watched after watching the movie, a user might have watched the movie because of other reasons (likes the actor, director, etc.). To personalize recommendation, other features need to considered such as genre, actors, director, producer, etc along with movies for generating association rules.</p>\n",
    "<p><b>3) Empty Matches: </b> Even if shopping history of a user is not empty, there might be no association rules because of high min_support and high min_confidence. In this case, lower the min_support and min_confidence until atleast one rule is formed based on user's history.</p>\n",
    "<p><b>4)Partial Matches: </b> The user might not have purchased all the movies of the antecedent rule. For example, if user's history is (a, b) and the rule is (a,b,c) -> (d), then instead of returning empty recommendations, consider recommending the rule based on confidence threshold</p>\n",
    "<p><b>5) Lack of Diversity: </b>As seen in above runs, top rules usually contain a franchise movies making the recommendation catering to a specific type of movies. To address this, group the movies of the franchise as one, example, all star wars movies can be considered as one movie, and recommend movies based on groups. This leads to more diverse recommendation.</p>\n",
    "<p><b>6) New Movies: </b> Since recommendation is based on past movies, new movies will not be recommended. To solve this problem, consider using additional rules as suggested before such as making use of rules generated with genre, actors, etc., and assign weights for feature rules.</p>\n",
    "<p><b>7) Cost Limit: </b> Along with user's shopping history, order history needs to be also considered such as cost, number of movies, etc. Then recommend the movies that are in the purchasing range of the user. </p>\n",
    "<p><b>8) Time factor: </b>As known, rules are formed based on entire history, some rules might not be relevant. For example, some people have long shopping histories and some have short. So, a rule formed might be relevant to a person with long history but not to a person with short history (ex: a person who has been watching movies from 1990 to 2024 might want to get recommendation of 1990's movies where as a person who has been watching movies from 2010 might not want to watch a 1990's movie). Consider decreasing weights to rules if newer shopping carts do not have the movies.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
